{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "309bafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab842f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d42aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a35790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dce2ca",
   "metadata": {},
   "source": [
    "# Web Scraping part\n",
    "\n",
    ". Using BeautifulSoup to extract the article title and content data \n",
    ". Concatenating both the title and content and returning from the function\n",
    ". New column 'ARTICLE' is created with the article data of all the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_text(url):\n",
    "    paragraphs=[]\n",
    "    data = requests.get(url,headers={\"User-Agent\": \"XY\"}).text\n",
    "    soup = BeautifulSoup(data,'lxml')\n",
    "    title=soup.find_all('h1')[0].text.strip()\n",
    "    p=soup.find_all('p')\n",
    "    for paragraph in p:\n",
    "        paragraph=paragraph.text.strip()\n",
    "        paragraphs.append(paragraph)\n",
    "    text=''.join(paragraphs)\n",
    "    text=title+\" \"+text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['URL'][36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04c67d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_text(urls_df['URL'][36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_text(urls_df['URL'][37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df.drop(37, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b16632",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['ARTICLE'] = urls_df['URL'].apply(url_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a403e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['ARTICLE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11b055",
   "metadata": {},
   "source": [
    "Saving the extracted text into individual text files with URL_ID as filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls_df.index:\n",
    "    filename=str(urls_df['URL_ID'][i])\n",
    "    text=str(urls_df['ARTICLE'][i])\n",
    "    path='articles/'+filename+'.txt'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        file = open(path,'w',encoding='utf-8')\n",
    "        file.write(text)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47757ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer , sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273c3e4",
   "metadata": {},
   "source": [
    "# Loading positive and negative words as well as stopwords from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MasterDictionary/positive-words.txt','r') as file:\n",
    "    positive_words = file.read().lower()\n",
    "positive_words = positive_words.split('\\n')\n",
    "#positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MasterDictionary/negative-words.txt','r', errors='ignore') as file:\n",
    "    negative_words = file.read().lower()\n",
    "negative_words = negative_words.split('\\n')\n",
    "#negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('StopWords/StopWords_Auditor.txt','r') as file:\n",
    "    stopwords_auditor = file.read().lower()\n",
    "stopwords_auditor = stopwords_auditor.split('\\n')\n",
    "stopwords_auditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825718a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('StopWords/StopWords_Currencies.txt','r', errors='ignore') as file:\n",
    "    stopwords_currencies = file.read().lower()\n",
    "stopwords_currencies = stopwords_currencies.split('\\n')\n",
    "stopwords_currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59591e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_currency = []\n",
    "for currency in stopwords_currencies:\n",
    "    stopwords_currency.append(currency.split('|')[0].strip())\n",
    "stopwords_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2120b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('StopWords/StopWords_DatesandNumbers.txt','r') as file:\n",
    "    stopwords_datesandnumbers = file.read().lower()\n",
    "stopwords_datesandnumbers = stopwords_datesandnumbers.split('\\n')\n",
    "stopwords_datesandnumber = []\n",
    "for date in stopwords_datesandnumbers:\n",
    "    if \"|\" in date:\n",
    "        stopwords_datesandnumber.append(date.split(\"|\")[0].strip())\n",
    "    else:\n",
    "        stopwords_datesandnumber.append(date.strip())\n",
    "stopwords_datesandnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d18583",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('StopWords/StopWords_Generic.txt','r') as file:\n",
    "    stopwords_generic = file.read().lower()\n",
    "stopwords_generic = stopwords_generic.split('\\n')\n",
    "stopwords_generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('StopWords/StopWords_GenericLong.txt','r') as file:\n",
    "    stopwords_genericlong = file.read().lower()\n",
    "stopwords_genericlong = stopwords_genericlong.split('\\n')\n",
    "stopwords_genericlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62785f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('StopWords/StopWords_Geographic.txt','r') as file:\n",
    "    stopwords_geo = file.read().lower()\n",
    "stopwords_geo = stopwords_geo.split('\\n')\n",
    "stopwords_geographic = []\n",
    "for geo in stopwords_geo:\n",
    "    if \"|\" in geo:\n",
    "        stopwords_geographic.append(geo.split('|')[0].strip())\n",
    "    else:\n",
    "        stopwords_geographic.append(geo.strip())\n",
    "stopwords_geographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('StopWords/StopWords_Names.txt','r') as file:\n",
    "    stopwords_names = file.read().lower()\n",
    "stopwords_names = stopwords_names.split('\\n')\n",
    "stopwords_name = []\n",
    "for name in stopwords_names:\n",
    "    if \"|\" in name:\n",
    "        stopwords_name.append(name.split('|')[0].strip())\n",
    "    else:\n",
    "        stopwords_name.append(name.strip())\n",
    "stopwords_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "stop_words = stopwords_auditor + stopwords_currency + stopwords_datesandnumber + stopwords_generic + stopwords_genericlong + stopwords_geographic + stopwords_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049380ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_words;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02e41a",
   "metadata": {},
   "source": [
    "# Defining functions for calculating required variables after analysis\n",
    "\n",
    "- RegexpTokenizer() is used to tokenize the article text, the stopwords in these tokens are filtered out. After filtering, the filtered tokens are then used to conduct the analysis on the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenz(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = str(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filter = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            filter.append(token)\n",
    "    return filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15eccf5",
   "metadata": {},
   "source": [
    "Positive Score: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_score(text):\n",
    "    tokens = tokenz(text)\n",
    "    pos_score = 0\n",
    "    for token in tokens:\n",
    "        if token in positive_words:\n",
    "            pos_score += 1\n",
    "    return pos_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36c5db",
   "metadata": {},
   "source": [
    "Negative Score: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f892fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_score(text):\n",
    "    tokens = tokenz(text)\n",
    "    neg_score = 0\n",
    "    for token in tokens:\n",
    "        if token in negative_words:\n",
    "            neg_score -= 1\n",
    "    return neg_score * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f900398",
   "metadata": {},
   "source": [
    "Polarity Score: This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: \n",
    "Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "Range is from -1 to +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_score(pos_score,neg_score):\n",
    "    polar_score = (pos_score - neg_score)/((pos_score + neg_score)+0.000001)\n",
    "    return polar_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ffc10",
   "metadata": {},
   "source": [
    "Subjectivity Score: This is the score that determines if a given text is objective or subjective. It is calculated by using the formula: \n",
    "Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "Range is from 0 to +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity_score(pos_score,neg_score,text):\n",
    "    tokens = tokenz(text)\n",
    "    sub_score = (pos_score + neg_score)/((len(tokens)) + 0.000001)\n",
    "    return sub_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7ef85",
   "metadata": {},
   "source": [
    "Average Sentence Length = the number of words / the number of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98617855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_length(text):\n",
    "    words = len(tokenz(text))\n",
    "    sentences = len(text.split('.'))\n",
    "    avg_length = words / sentences\n",
    "    return avg_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b279b5",
   "metadata": {},
   "source": [
    "Percentage of Complex words = the number of complex words / the number of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_complex(text):\n",
    "    tokens = tokenz(text)\n",
    "    complex_cnt = 0\n",
    "    for token in tokens:\n",
    "        vowels = 0\n",
    "        if token.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for l in token:\n",
    "                if (l == 'a' or l == 'e' or l == 'i' or l == 'o' or l == 'u'):\n",
    "                    vowels += 1\n",
    "            if vowels > 2:\n",
    "                complex_cnt += 1\n",
    "    if len(token) != 0:\n",
    "        return complex_cnt/len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e7372",
   "metadata": {},
   "source": [
    "Complex words are words in the text that contain more than two syllables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d562153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_word_count(text):\n",
    "    tokens = tokenz(text)\n",
    "    complex_cnt = 0\n",
    "    for token in tokens:\n",
    "        vowels = 0\n",
    "        if token.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for l in token:\n",
    "                if (l == 'a' or l == 'e' or l == 'i' or l == 'o' or l == 'u'):\n",
    "                    vowels += 1\n",
    "            if vowels > 2:\n",
    "                complex_cnt += 1\n",
    "    return complex_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9cac82",
   "metadata": {},
   "source": [
    "Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog_index(avg_sent_length,percentage_complex):\n",
    "    return 0.4 * (avg_sent_length + percentage_complex)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de461d25",
   "metadata": {},
   "source": [
    "We count the total cleaned words present in the text by \n",
    "removing the stop words \n",
    "removing any punctuations like ? ! , . from the word before counting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc39912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    cnt = len(tokenz(text))\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bbfc4",
   "metadata": {},
   "source": [
    "We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9423468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count_word(text):\n",
    "    words = tokenz(text)\n",
    "    words_cnt = len(words)\n",
    "    vowels = 0\n",
    "    for word in words:\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for l in word:\n",
    "                if (l == 'a' or l == 'e' or l == 'i' or l == 'o' or l == 'u'):\n",
    "                    vowels += 1\n",
    "    return vowels / words_cnt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86f6ca",
   "metadata": {},
   "source": [
    "To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken so that the country name US is not included in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef807a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_personal_pronouns(text):\n",
    "    words = tokenz(text)\n",
    "    pronoun_re = r'\\b(I|my|we|us|ours)\\b'\n",
    "    matches = re.findall(pronoun_re,text)\n",
    "    return len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b0684",
   "metadata": {},
   "source": [
    "Average Word Length is calculated by the formula:\n",
    "Sum of the total number of characters in each word/Total number of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44133d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(text):\n",
    "    words = tokenz(text)\n",
    "    charcnt = 0\n",
    "    for word in words:\n",
    "        charcnt += len(word.strip())\n",
    "    if len(words) != 0:\n",
    "        return charcnt / len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6759fd",
   "metadata": {},
   "source": [
    "# Applying all the functions to the dataframe for respective new columns for the calculated variables\n",
    "\n",
    "- for functions with inputs from more than one columns in the dataframe, np.vectorize() is used. For the rest pd.apply() is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c51754",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['POSITIVE SCORE'] = urls_df['ARTICLE'].apply(positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bec963",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['NEGATIVE SCORE'] = urls_df['ARTICLE'].apply(negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc186e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['POLARITY SCORE'] = np.vectorize(polarity_score)(urls_df['POSITIVE SCORE'],urls_df['NEGATIVE SCORE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f238d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['SUBJECTIVITY SCORE'] = np.vectorize(subjectivity_score)(urls_df['POSITIVE SCORE'],urls_df['NEGATIVE SCORE'],urls_df['ARTICLE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['AVG SENTENCE LENGTH'] = urls_df['ARTICLE'].apply(avg_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011d347",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75477fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['PERCENTAGE OF COMPLEX WORDS'] = urls_df['ARTICLE'].apply(perc_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['FOG INDEX'] = np.vectorize(fog_index)(urls_df['AVG SENTENCE LENGTH'],urls_df['PERCENTAGE OF COMPLEX WORDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aad30c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_df['AVG NUMBER OF WORDS PER SENTENCE'] = urls_df['ARTICLE'].apply(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['COMPLEX WORD COUNT'] = urls_df['ARTICLE'].apply(complex_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d067b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['WORD COUNT'] = urls_df['ARTICLE'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bafbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['SYLLABLE PER WORD'] = urls_df['ARTICLE'].apply(syllable_count_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['PERSONAL PRONOUNS'] = urls_df['ARTICLE'].apply(cal_personal_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5094a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df['AVG WORD LENGTH'] = urls_df['ARTICLE'].apply(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df.drop('ARTICLE',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5639b",
   "metadata": {},
   "source": [
    "# Loading all the calculated variables from the dataframe into the given output excel file in the same structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df.to_excel(\"Output Data Structure.xlsx\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e338787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
